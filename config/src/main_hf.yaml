data:
  data_path: data/nq/parametric_relevance_tagged/validation.json
  max_seq_length: 1024
  use_single_context: True

model:
  model_name: meta-llama/Llama-3.1-8B-Instruct
  gen_kwargs:
    do_sample: False
    temperature: 1.0
    top_p: 1
    top_k: null
    max_new_tokens: 32
  prune:
    ratio: 0.3

judger:
  llm_model_name: meta-llama/Llama-3.1-8B-Instruct
  prompt_name: base
  cache_dir: src/cache_dir
  use_cache: True
  use_openai: False

self_task_prompt_name: base
generate_prompt_name: base
output_dir: src/results/main/e2e_llama-3.1
task: base
experiment_name: